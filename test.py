# test.py
import os
import torch
import torch.nn as nn
import numpy as np
import random
from pathlib import Path
from collections import OrderedDict
from copy import deepcopy

from options.test_options import TestOptions
from validate import validate
from networks.LaDeDa import LaDeDa9
from networks.Tiny_LaDeDa import tiny_ladeda


# ==================== è®¾ç½®éšæœºç§å­ ====================
def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± Random seed set to: {seed}")


# ==================== åŠ è½½æ¨¡å‹ ====================
def load_model(model_path):
    """
    æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨åˆ¤æ–­æ¨¡å‹ç±»å‹å¹¶åŠ è½½
    """
    model_name = Path(model_path).stem

    # åˆ¤æ–­æ˜¯å¦ä¸º Tiny æ¨¡å‹
    is_tiny = 'Tiny' in model_name

    # åˆ¤æ–­è®­ç»ƒæ•°æ®é›†
    trained_on = 'WildRF' if 'WildRF' in model_name else 'ForenSynth'

    print(f"\n{'=' * 60}")
    print(f"ğŸ“¦ Loading Model: {model_name}")
    print(f"   Type: {'Tiny-LaDeDa' if is_tiny else 'LaDeDa9'}")
    print(f"   Trained on: {trained_on}")
    print(f"{'=' * 60}")

    # åˆ›å»ºæ¨¡å‹
    if is_tiny:
        features_dim = 8
        model = tiny_ladeda(num_classes=1, preprocess_type='NPR')
    else:
        features_dim = 2048
        model = LaDeDa9(num_classes=1, preprocess_type='NPR')

    # åŠ è½½æƒé‡
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ Model file not found: {model_path}")

    print(f"ğŸ“¥ Loading weights from: {model_path}")
    state_dict = torch.load(model_path, map_location='cpu')

    # å¤„ç†å¯èƒ½çš„æ ¼å¼
    if isinstance(state_dict, dict):
        if 'model' in state_dict:
            state_dict = state_dict['model']
        elif 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']

    # æ¸…ç†state_dictçš„key
    cleaned_state_dict = OrderedDict()
    for k, v in state_dict.items():
        new_key = k.replace('module.', '') if k.startswith('module.') else k
        cleaned_state_dict[new_key] = deepcopy(v)

    # åŠ è½½åˆ°æ¨¡å‹
    try:
        model.load_state_dict(cleaned_state_dict, strict=True)
        print("âœ… Model loaded successfully!")
    except Exception as e:
        print(f"âš ï¸  Warning during loading: {e}")
        print("   Trying with strict=False...")
        model.load_state_dict(cleaned_state_dict, strict=False)

    model.eval()
    model.cuda()

    return model, features_dim, trained_on


# ==================== æµ‹è¯•WildRF ====================
def test_wildrf(model, opt):
    """
    æµ‹è¯•WildRFæ•°æ®é›†ï¼ˆReddit, Facebook, Twitterï¼‰
    """
    print(f"\n{'=' * 60}")
    print("ğŸ§ª Testing on WildRF Dataset")
    print(f"{'=' * 60}")

    # âœ… è‡ªåŠ¨å¤„ç†è·¯å¾„
    if hasattr(opt, 'dataroot') and opt.dataroot:
        dataroot = opt.dataroot
    else:
        dataroot = './datasets/WildRF/test'

    # å¦‚æœdatarootä¸ä»¥testç»“å°¾ï¼Œè‡ªåŠ¨æ·»åŠ 
    if not dataroot.endswith('/test') and not dataroot.endswith('/test/'):
        if 'WildRF' in dataroot and not os.path.exists(os.path.join(dataroot, 'reddit')):
            dataroot = os.path.join(dataroot, 'test')

    print(f"ğŸ“ Data root: {dataroot}")

    platforms = ['reddit', 'facebook', 'twitter']
    results = {}
    accs, aps, aucs = [], [], []

    for platform in platforms:
        platform_path = os.path.join(dataroot, platform)

        if not os.path.exists(platform_path):
            print(f"âš ï¸  {platform} not found at {platform_path}, skipping...")
            continue

        print(f"\nğŸ“Š Evaluating {platform.upper()}...")

        # è®¾ç½®éªŒè¯å‚æ•°
        opt.dataroot = platform_path
        opt.classes = ['']
        opt.no_resize = False
        opt.no_crop = True
        opt.is_aug = False

        try:
            acc, ap, r_acc, f_acc, auc, precision, recall = validate(model, opt)

            results[platform] = {
                'ACC': acc,
                'AP': ap,
                'AUC': auc,
                'Real_ACC': r_acc,
                'Fake_ACC': f_acc,
                'Precision': precision,
                'Recall': recall
            }

            accs.append(acc)
            aps.append(ap)
            aucs.append(auc)

            print(f"   âœ… ACC: {acc * 100:5.1f}% | AP: {ap * 100:5.1f}% | AUC: {auc * 100:5.1f}%")

        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            continue

    # æ‰“å°æ±‡æ€»
    if len(results) > 0:
        print(f"\n{'-' * 60}")
        print("ğŸ“ˆ WildRF Results Summary")
        print(f"{'-' * 60}")
        print(f"{'Platform':<12} | {'ACC (%)':>8} | {'AP (%)':>8} | {'AUC (%)':>8}")
        print(f"{'-' * 60}")

        for platform, metrics in results.items():
            print(f"{platform.upper():<12} | "
                  f"{metrics['ACC'] * 100:>8.2f} | "
                  f"{metrics['AP'] * 100:>8.2f} | "
                  f"{metrics['AUC'] * 100:>8.2f}")

        print(f"{'-' * 60}")
        print(f"{'MEAN':<12} | "
              f"{np.mean(accs) * 100:>8.2f} | "
              f"{np.mean(aps) * 100:>8.2f} | "
              f"{np.mean(aucs) * 100:>8.2f}")
        print(f"{'=' * 60}")


# ==================== æµ‹è¯•ForenSynth ====================
def test_forensynth(model, opt):
    """
    æµ‹è¯•ForenSynthæ•°æ®é›†
    """
    print(f"\n{'=' * 60}")
    print("ğŸ§ª Testing on ForenSynth Dataset")
    print(f"{'=' * 60}")

    dataroot_forensynth = './datasets/CNNDetection/test'

    if not os.path.exists(dataroot_forensynth):
        print(f"âŒ ForenSynth test data not found at {dataroot_forensynth}, skipping.")
        return {}

    print(f"ğŸ“ Data root: {dataroot_forensynth}")

    generators = ['progan', 'biggan', 'stylegan', 'stylegan2',
                  'cyclegan', 'stargan', 'gaugan', 'deepfake']

    results = {}
    accs, aps, aucs = [], [], []

    for gen in generators:
        gen_path = os.path.join(dataroot_forensynth, gen)

        if not os.path.exists(gen_path):
            print(f"âš ï¸  {gen} not found, skipping...")
            continue

        print(f"\nğŸ“Š Evaluating {gen.upper()}...")

        # âœ… æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—ç»“æ„
        subdirs = os.listdir(gen_path)
        has_binary_structure = '0_real' in subdirs and '1_fake' in subdirs

        if has_binary_structure:
            # ç»“æ„1: gen/0_real, gen/1_fake (BigGAN, StarGAN, GauGAN)
            print(f"   ğŸ“ Direct binary structure")
            opt.dataroot = gen_path
            opt.classes = ['']
            opt.no_resize = False
            opt.no_crop = True
            opt.is_aug = False

            try:
                acc, ap, r_acc, f_acc, auc, precision, recall = validate(model, opt)

                results[gen] = {'ACC': acc, 'AP': ap, 'AUC': auc}
                accs.append(acc)
                aps.append(ap)
                aucs.append(auc)

                print(f"   âœ… ACC: {acc * 100:5.1f}% | AP: {ap * 100:5.1f}%")
            except Exception as e:
                print(f"   âŒ Error: {e}")
                continue
        else:
            # ç»“æ„2: gen/category/0_real, gen/category/1_fake (ProGAN, StyleGAN, etc.)
            print(f"   ğŸ“ Nested category structure with {len(subdirs)} categories")

            # âœ… å¯¹æ¯ä¸ªcategoryåˆ†åˆ«æµ‹è¯•ï¼Œç„¶åå¹³å‡
            category_accs, category_aps = [], []

            for category in subdirs:
                category_path = os.path.join(gen_path, category)
                if not os.path.isdir(category_path):
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰ 0_real å’Œ 1_fake
                if not (os.path.exists(os.path.join(category_path, '0_real')) and
                        os.path.exists(os.path.join(category_path, '1_fake'))):
                    continue

                opt.dataroot = category_path
                opt.classes = ['']
                opt.no_resize = False
                opt.no_crop = True
                opt.is_aug = False

                try:
                    acc, ap, r_acc, f_acc, auc, precision, recall = validate(model, opt)
                    category_accs.append(acc)
                    category_aps.append(ap)
                except Exception as e:
                    print(f"      âš ï¸  Error on {category}: {e}")
                    continue

            if len(category_accs) > 0:
                avg_acc = np.mean(category_accs)
                avg_ap = np.mean(category_aps)

                results[gen] = {'ACC': avg_acc, 'AP': avg_ap, 'AUC': avg_acc}
                accs.append(avg_acc)
                aps.append(avg_ap)
                aucs.append(avg_acc)

                print(
                    f"   âœ… ACC: {avg_acc * 100:5.1f}% | AP: {avg_ap * 100:5.1f}% (avg over {len(category_accs)} categories)")

        # æ‰“å°æ±‡æ€»
    if len(results) > 0:
        print(f"\n{'-' * 60}")
        print("ğŸ“ˆ ForenSynth Results Summary")
        print(f"{'-' * 60}")
        print(f"{'Generator':<14} | {'ACC (%)':>8} | {'AP (%)':>8}")
        print(f"{'-' * 60}")

        for gen, metrics in results.items():
            print(f"{gen.upper():<14} | "
                  f"{metrics['ACC'] * 100:>8.2f} | "
                  f"{metrics['AP'] * 100:>8.2f}")

        print(f"{'-' * 60}")
        print(f"{'MEAN':<14} | "
              f"{np.mean(accs) * 100:>8.2f} | "
              f"{np.mean(aps) * 100:>8.2f}")
        print(f"{'=' * 60}")


# ==================== ä¸»å‡½æ•° ====================
def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    # è§£æå‚æ•°
    opt = TestOptions().parse(print_options=False)

    # æ£€æŸ¥model_path
    if not hasattr(opt, 'model_path') or not opt.model_path:
        raise ValueError("âŒ Please specify --model_path")

    # åŠ è½½æ¨¡å‹
    model, features_dim, trained_on = load_model(opt.model_path)

    if hasattr(opt, 'dataset') and opt.dataset:
        test_on = opt.dataset.lower()  # âœ… ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        print(f"ğŸ“Œ Using command line dataset: {test_on}")
    else:
        # åªæœ‰æ²¡æœ‰æŒ‡å®šæ—¶æ‰æ ¹æ®æ¨¡å‹ååˆ¤æ–­
        test_on = 'wildrf' if 'WildRF' in opt.model_path else 'forensynth'
        print(f"ğŸ“Œ Auto-detected dataset: {test_on}")

    print(f"\nğŸ¯ Test Configuration:")
    print(f"   Model: {Path(opt.model_path).name}")
    print(f"   Features Dim: {features_dim}")
    print(f"   Test Dataset: {test_on.upper()}")

    # æ‰§è¡Œæµ‹è¯•
    if test_on == 'wildrf':
        results = test_wildrf(model, opt)
    elif test_on == 'forensynth':
        results = test_forensynth(model, opt)
    elif test_on == 'both':
        print("\nğŸ”„ Testing on both datasets...")
        wildrf_results = test_wildrf(model, opt)
        forensynth_results = test_forensynth(model, opt)
    else:
        raise ValueError(f"Unknown dataset: {test_on}")

    print("\nâœ… Testing completed!")


if __name__ == '__main__':
    main()